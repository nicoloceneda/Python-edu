{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow - Computation Graphs and Gradients.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpTKIzTFKhCw9Miqc4A77K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoloceneda/Python-edu/blob/master/TensorFlow_Computation_Graphs_and_Gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyDf6fDaOh-o",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow - Computation Graphs and Gradients\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjeDtabOOprs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jXJ2uehOrkF",
        "colab_type": "text"
      },
      "source": [
        "## Creating a computation graph\n",
        "TensorFlow relies on computation graphs to perform computations and derive the relationship between tensors, from the input all the way to the output. A computation graph is a network of nodes, where each node represents a tensor or an operation, which applies a function to its input tensor(s) and returns zero or more output tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IeVoJnbyVajR"
      },
      "source": [
        "### TensorFlow v1.x\n",
        "TensorFlow v1.x's low-level API uses **static computation graphs**, which must be explicitly defined, although this is not trivial for large and complex models. The steps for building, compiling and evaluating a graph are:\n",
        "\n",
        "1. Instantiate an empty graph using `tf.Graph()`\n",
        "2. Add nodes to the computation graph using `g.as_default()`\n",
        "3. Evaluate the graph: \\\n",
        "     3.1 Start a new session using `tf.compat.v1.Session()` \\\n",
        "     3.2 Initialize the variables using `tf.compat.v1.global_variables_initializer()` \\\n",
        "     3.3 Execute the operations using `sess.run()` \\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "65c933b8-1ea5-4ae3-8445-276be195f32f",
        "id": "-CKP9taSVZAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example 1: only constants\n",
        "g = tf.Graph()\n",
        "\n",
        "with g.as_default():\n",
        "  a = tf.constant(1, name='a')\n",
        "  b = tf.constant(2, name='b')\n",
        "  c = tf.constant(3, name='c')\n",
        "  z = 2 * (a - b) + c\n",
        "\n",
        "with tf.compat.v1.Session(graph=g) as sess:\n",
        "  print('z =', sess.run(z))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e1fafe6e-f040-485a-90cc-3c60b18585e3",
        "id": "b74EBvgEVYZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example 2: only placeholders\n",
        "g = tf.Graph()\n",
        "\n",
        "with g.as_default():\n",
        "  a = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_a')\n",
        "  b = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_b')\n",
        "  c = tf.compat.v1.placeholder(shape=None, dtype=tf.int32, name='tf_c')\n",
        "  z = 2 * (a - b) + c\n",
        "\n",
        "with tf.compat.v1.Session(graph=g) as sess:\n",
        "  print('z =', sess.run(z, feed_dict = {a: 1, b: 2, c: 3}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6be7f278-42e4-48cf-cc4a-26e2592fea6f",
        "id": "SechOWZoVW1B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Example 3: placeholders and variables\n",
        "g = tf.Graph()\n",
        "\n",
        "with g.as_default():\n",
        "  x = tf.compat.v1.placeholder(shape=None, dtype=tf.float32, name='x')\n",
        "  w = tf.Variable(2.0, name='weight')\n",
        "  b = tf.Variable(0.7, name='bias')\n",
        "  z = w * x + b\n",
        "\n",
        "  init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "with tf.compat.v1.Session(graph=g) as sess:\n",
        "  sess.run(init)\n",
        "  for t in [1.0, 0.6, -1.8]:\n",
        "    print('x = {:>5.2f} --> z = {:>5.2f}'.format(t, sess.run(z, feed_dict={x: t})))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "x =  1.00 --> z =  2.70\n",
            "x =  0.60 --> z =  1.90\n",
            "x = -1.80 --> z = -2.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yx6xeN36VVoF"
      },
      "source": [
        "### TensorFlow v2\n",
        "TensorFlow v2 uses **dynamic computation graphs** (also called eager execution), which allow to evaluate operations on the fly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Afhx37cp1HV",
        "colab_type": "code",
        "outputId": "e8cc77af-4945-48e4-ad13-b1070f8b0d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example 1 from TensorFlow v1.x\n",
        "a = tf.constant(1, name='a')\n",
        "b = tf.constant(2, name='b')\n",
        "c = tf.constant(3, name='c')\n",
        "z = 2 * (a - b) + c\n",
        "\n",
        "tf.print('z =', z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewMrvNnxxOyW",
        "colab_type": "code",
        "outputId": "b76af8f8-cc1c-4d98-d7e5-b0e372cc8e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example 2 from TensorFlow v1.x\n",
        "def compute_z(a, b, c):\n",
        "  z = tf.add(tf.multiply(2, tf.subtract(a, b)), c)\n",
        "  return z\n",
        "\n",
        "z = compute_z(1, 2, 3)\n",
        "tf.print('z =', z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--eRGLbsgRUT",
        "colab_type": "code",
        "outputId": "95b7a8df-1648-4c65-b83d-8219c96d6ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Example 3 from TensorFlow v1.x\n",
        "def compute_z(x):\n",
        "  w = tf.Variable(2.0)\n",
        "  b = tf.Variable(0.7)\n",
        "  return tf.add(tf.multiply(w, x), b)\n",
        "\n",
        "x = tf.constant(3.0)\n",
        "print('z =', compute_z(x).numpy())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z = 6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU5daJn99OJQ",
        "colab_type": "text"
      },
      "source": [
        "Since dynamic graphs are not as computationally efficient as static ones, TensorFlow v2 automatically compiles Python code into a static graph using a tool called AutoGraph. \n",
        "\n",
        "Compile a **function into a static graph** using the `@tf.function` decorator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWESrk9SCtTi",
        "colab_type": "code",
        "outputId": "f533c788-7313-44ed-e853-0ced9f644821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "@tf.function\n",
        "def compute_z(a, b, c):\n",
        "  z = tf.add(tf.multiply(2, tf.subtract(a, b)), c)\n",
        "  return z\n",
        "\n",
        "z = compute_z(1, 2, 3)\n",
        "print('z =', z.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BPGQr-aqAvE",
        "colab_type": "text"
      },
      "source": [
        "**Limit the ways to call** a function using `tf.TensorSpec`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3x4Z_B0NqXD",
        "colab_type": "code",
        "outputId": "9688a9e4-8fac-425a-cc12-a6079f6e69ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32), \n",
        "                              tf.TensorSpec(shape=[None], dtype=tf.int32), \n",
        "                              tf.TensorSpec(shape=[None], dtype=tf.int32),))\n",
        "def compute_z(a, b, c):\n",
        "  z = tf.add(tf.multiply(2, tf.subtract(a, b)), c)\n",
        "  return z\n",
        "\n",
        "# Calling the function using tensors with rank 1 or lists that can be converted to rank 1 tensors:\n",
        "tf.print('Rank 1 inputs:', compute_z([1], [2], [3]))\n",
        "tf.print('Rank 1 inputs:', compute_z([1, 2], [2, 4], [3, 6]))\n",
        "\n",
        "# Calling the function using tensors with ranks other than 1 will result in errors:\n",
        "# tf.print('Rank 0 inputs:', compute_z(1, 2, 3)) \n",
        "# tf.print('Rank 2 Inputs:', compute_z([[1], [2]], [2], [4]], [[3], [6]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rank 1 inputs: [1]\n",
            "Rank 1 inputs: [1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6LWMvYo9wCv",
        "colab_type": "text"
      },
      "source": [
        "To use a **variable inside a decorated function**, define the variable outside of the decorated function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvd4Zgq29wcm",
        "colab_type": "code",
        "outputId": "cd6459b0-a550-41e5-b4cb-299c4b9f6188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "w = tf.Variable(tf.random.uniform((2, 3)))\n",
        "\n",
        "@tf.function\n",
        "def compute_z(x):\n",
        "  return tf.matmul(w, x)\n",
        "\n",
        "x = tf.constant([[1], [2], [3]], dtype=tf.float32)\n",
        "print(compute_z(x).numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.8468504]\n",
            " [5.1717687]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvZhH50RKfs0",
        "colab_type": "text"
      },
      "source": [
        "## Computing gradients\n",
        "Optimizing neural networks via algorithms such as stochastic gradient descent requires computing the gradients of the cost with respect to the weights. TensorFlow supports automatic differentiation, which represents a set of computational techniques for computing derivatives or gradients of arithmetic operations. TensorFlow uses computation graphs to compute gradients.\n",
        "\n",
        "Compute the **gradient wrt a trainable variable** using `tf.GradientTape`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqS70NhcXdtz",
        "colab_type": "code",
        "outputId": "66b94c01-8ccc-499d-82f2-bc8a69326b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w = tf.Variable(1.0, trainable=True)\n",
        "b = tf.Variable(0.5, trainable=True)\n",
        "\n",
        "x = tf.constant([1.4])\n",
        "y = tf.constant([2.1])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = tf.add(tf.multiply(w, x), b)\n",
        "  loss = tf.reduce_sum(tf.square(tf.subtract(y, z)))\n",
        "\n",
        "dloss_dw = tape.gradient(loss, w)\n",
        "print('dL/dw =', dloss_dw.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dL/dw = -0.55999976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdo59fQ7z3Ss",
        "colab_type": "text"
      },
      "source": [
        "Compute the **gradient wrt a non-trainable variable** using `tf.GradientTape` and `tape.watch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv8unfCt0i-o",
        "colab_type": "code",
        "outputId": "12dac689-133e-42ce-f35a-847ee3a5ee8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w = tf.Variable(1.0, trainable=True)\n",
        "b = tf.Variable(0.5, trainable=True)\n",
        "\n",
        "x = tf.constant([1.4])\n",
        "y = tf.constant([2.1])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  z = tf.add(tf.multiply(w, x), b)\n",
        "  loss = tf.reduce_sum(tf.square(tf.subtract(y, z)))\n",
        "\n",
        "dloss_dx = tape.gradient(loss, x)\n",
        "print('dL/dx =', dloss_dx.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dL/dx = [-0.39999986]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKDu9fk_HMx1",
        "colab_type": "text"
      },
      "source": [
        "When we monitor computations with `tf.GradientTape`, by default the tape keeps the resources only for a single gradient computation: after calling `tape.gradient` once, the tape is cleared. \n",
        "\n",
        "Compute **more than one gradient** making the tape `persistent`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e70u2J7fHxjk",
        "colab_type": "code",
        "outputId": "05001197-ab91-4f97-e366-349ec6c8e182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "w = tf.Variable(1.0, trainable=True)\n",
        "b = tf.Variable(0.5, trainable=True)\n",
        "\n",
        "x = tf.constant([1.4])\n",
        "y = tf.constant([2.1])\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  z = tf.add(tf.multiply(w, x), b)\n",
        "  loss = tf.reduce_sum(tf.square(tf.subtract(y, z)))\n",
        "\n",
        "dloss_dw = tape.gradient(loss, w)\n",
        "print('dL/dw =', dloss_dw.numpy())\n",
        "\n",
        "dloss_db = tape.gradient(loss, b)\n",
        "print('dL/db =', dloss_db.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dL/dw = -0.55999976\n",
            "dL/db = -0.39999986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH8y4uxzJ3Ie",
        "colab_type": "text"
      },
      "source": [
        "**Define an optimizer** and apply the gradients to optimize the model parameters using `tf.keras.optimizers`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dw_VJWSKBzN",
        "colab_type": "code",
        "outputId": "68c54d28-3b7b-4929-b745-3d008f75b58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "w = tf.Variable(1.0, trainable=True)\n",
        "b = tf.Variable(0.5, trainable=True)\n",
        "\n",
        "x = tf.constant([1.4])\n",
        "y = tf.constant([2.1])\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  z = tf.add(tf.multiply(w, x), b)\n",
        "  loss = tf.reduce_sum(tf.square(tf.subtract(y, z)))\n",
        "\n",
        "dloss_dw = tape.gradient(loss, w)\n",
        "dloss_db = tape.gradient(loss, b)\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "optimizer.apply_gradients(zip([dloss_dw, dloss_db], [w, b]))\n",
        "print('Updated w:', w.numpy())\n",
        "print('Updated b:', b.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated w: 1.0056\n",
            "Updated b: 0.504\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}